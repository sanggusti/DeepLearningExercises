# intermediate representations

- Like a shared dialect of neural network layers
- Conv2d(TF) Convolution(Caffe) Conv(ONNX) are all convolution layer in an IR
- Have any desired optimizations performed
- Ready for use with the inference engine

IR (.xml or .bin file)

Intermediate Representations (IRs) are the OpenVINO™ Toolkit’s standard structure and naming for neural network architectures. A Conv2D layer in TensorFlow, Convolution layer in Caffe, or Conv layer in ONNX are all converted into a Convolution layer in an IR

The IR is able to be loaded directly into the Inference Engine, and is actually made of two output files from the Model Optimizer: an XML file and a binary file. The XML file holds the model architecture and other important metadata, while the binary file holds weights and biases in a binary format. You need both of these files in order to run inference Any desired optimizations will have occurred while this is generated by the Model Optimizer, such as changes to precision. You can generate certain precisions with the --data_type argument, which is usually FP32 by default.

The Model Optimizer works almost like a translator here, making the Intermediate Representation a shared dialect of all the supported frameworks, which can be understood by the Inference Engine.

links:
- You can find the main developer documentation on converting models in the OpenVINO™ Toolkit [here](https://docs.openvinotoolkit.org/2019_R3/_docs_MO_DG_prepare_model_convert_model_Converting_Model.html). We’ll cover how to do so with TensorFlow, Caffe and ONNX (useful for PyTorch) over the next several pages.

- You can find the documentation on different layer names when converted to an IR [here](https://docs.openvinotoolkit.org/2019_R3/_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html).

- Finally, you can find more in-depth data on each of the Intermediate Representation layers themselves [here](https://docs.openvinotoolkit.org/2019_R3/_docs_MO_DG_prepare_model_convert_model_IRLayersCatalogSpec.html).